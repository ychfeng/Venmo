{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "text preprocessing: token, remove stop words (I have Spacy worked out for lemma, attached code below)\n",
    "\n",
    "3 outputs: \n",
    "            1. number of classified (for each month)\n",
    "            2. average of each category's percentage across users (real time)\n",
    "            3. average of each category's percentage across users (user life time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different with the code for cluster\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import re\n",
    "\n",
    "from optparse import OptionParser\n",
    "\n",
    "import string\n",
    "from typing import Iterable\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP2\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"4G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def classify_Food(discription):\n",
    "    b=any([(x in new_dict['Food']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Food_udf=udf(classify_Food,IntegerType())\n",
    "\n",
    "def classify_Event(discription):\n",
    "    b=any([(x in new_dict['Event']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Event_udf=udf(classify_Event,IntegerType())\n",
    "\n",
    "def classify_People(discription):\n",
    "    b=any([(x in new_dict['People']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_People_udf=udf(classify_People,IntegerType())\n",
    "\n",
    "def classify_Activity(discription):\n",
    "    b=any([(x in new_dict['Activity']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Activity_udf=udf(classify_Activity,IntegerType())\n",
    "\n",
    "def classify_Travel(discription):\n",
    "    b=any([(x in new_dict['Travel']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Travel_udf=udf(classify_Travel,IntegerType())\n",
    "\n",
    "def classify_Transportation(discription):\n",
    "    b=any([(x in new_dict['Transportation']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Transportation_udf=udf(classify_Transportation,IntegerType())\n",
    "\n",
    "def classify_Utility(discription):\n",
    "    b=any([(x in new_dict['Utility']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Utility_udf=udf(classify_Utility,IntegerType())\n",
    "\n",
    "def classify_Cash(discription):\n",
    "    b=any([(x in new_dict['Cash']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Cash_udf=udf(classify_Cash,IntegerType())\n",
    "\n",
    "def classify_Illegal(discription):\n",
    "    b=any([(x in new_dict['Illegal/Sarcasm']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Illegal_udf=udf(classify_Illegal,IntegerType())\n",
    "\n",
    "# convert to dummy variable\n",
    "def convert_dummy(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "convert_dummy_udf=udf(convert_dummy,IntegerType())\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: venmo data\n",
    "\n",
    "inputFile = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/Users/yichuan/Desktop/Venmo project/data/venmoSample.csv\")\n",
    "inputFile = inputFile.na.fill('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user life time \n",
    "my_window = Window.partitionBy('user1')\n",
    "inputFile = inputFile.withColumn(\"min_date\", min(inputFile['datetime']).over(my_window))\n",
    "inputFile = inputFile.withColumn(\"diff_date\", datediff('datetime','min_date'))\n",
    "inputFile = inputFile.withColumn(\"customer_lifetime\", F.when(inputFile['diff_date']==0,0).otherwise(inputFile['diff_date']/30+1).cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+\n",
      "|user1|user2|transaction_type|           datetime|         description|is_business|            story_id|           min_date|diff_date|customer_lifetime|\n",
      "+-----+-----+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+\n",
      "| 2866|30588|         payment|2015-09-15 14:27:00|               Stuff|      false|55f82ab4cd03c9af2...|2015-09-15 14:27:00|        0|                0|\n",
      "| 6620| 6507|         payment|2012-04-16 14:32:43|for taking me out...|      false|54e4165dcd03c9af2...|2012-04-16 14:32:43|        0|                0|\n",
      "| 6620| 6606|         payment|2013-08-05 17:21:57|       Ralph LAU-ren|      false|51ffdb8f7de518fa3...|2012-04-16 14:32:43|      476|               16|\n",
      "|28170|27438|         payment|2012-11-03 02:35:06|           November!|      false|54e41951cd03c9af2...|2012-11-03 02:35:06|        0|                0|\n",
      "|28759|65628|         payment|2013-08-11 02:55:32|               Lyft!|      false|5206fd25d56b6bac5...|2013-03-13 16:39:28|      151|                6|\n",
      "+-----+-----+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputFile.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "\n",
    "punctuations = '~|`|\\!|@|#|$|%|^|&|\\*|\\(|\\)|-|\\+|=|_|\\{|\\}|\\[|\\]|;|:|\\?|\\.|,|<|>|/|\\'|\\\"'\n",
    "\n",
    "inputFile=inputFile\\\n",
    ".withColumn('description_rm_pun',regexp_replace(col('description'),punctuations, ' '))\n",
    "\n",
    "# keep emoji\n",
    "inputFile=inputFile\\\n",
    ".withColumn('description_emoji',regexp_replace(col('description_rm_pun'),'[\\w\\s]', ''))\n",
    "# keep text\n",
    "inputFile=inputFile\\\n",
    ".withColumn('description_word',regexp_replace(col('description_rm_pun'),'[^\\w\\s]', ''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = inputFile.withColumn(\"total_tokens\", countTokens(col(\"description\")))\n",
    "inputFile = inputFile.withColumn(\"year\", year(\"datetime\"))\n",
    "inputFile = inputFile.withColumn(\"month\", month(\"datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = RegexTokenizer().setInputCol(\"description_word\").setOutputCol(\"tokenized_words\").setPattern(\" \").setToLowercase(True)\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover().setStopWords(englishStopWords).setInputCol(\"tokenized_words\").setOutputCol(\"tokenized_words_filtered\")\n",
    "pipeline = Pipeline(stages=[rt, stops])\n",
    "\n",
    "# Fit the pipeline to dataframe\n",
    "pipelineFit = pipeline.fit(inputFile)\n",
    "inputFile = pipelineFit.transform(inputFile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### \n",
    "### if using Spacy, just replace the chunk right above with this one.\n",
    "###\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def text_clean(x):\n",
    "    '''\n",
    "    lemma + stopwords\n",
    "    '''\n",
    "    doc = nlp(x)\n",
    "    word_list = [token.lemma_ for token in doc]\n",
    "    words=[word for word in word_list if not nlp.vocab[word].is_stop and word.isalnum()]\n",
    "\n",
    "    return words\n",
    "udf_text_clean = udf(text_clean, ArrayType(StringType()))\n",
    "\n",
    "# lower text\n",
    "inputFile = inputFile.withColumn('lower_words', lower(col('description_word')))\n",
    "# lemma + stopwords\n",
    "inputFile = inputFile.withColumn(\"tokenized_words_filtered\", udf_text_clean(\"lower_words\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count tokens in each transaction\n",
    "inputFile = inputFile.withColumn(\"total_word_tokens\", countTokens(col(\"description_word\"))) \n",
    "inputFile = inputFile.withColumn(\"total_emoji_tokens\", countTokens(col(\"description_emoji\"))) \n",
    "inputFile = inputFile.withColumn(\"if_emoji_only\", inputFile['total_emoji_tokens'] == inputFile['total_tokens'])\n",
    "inputFile = inputFile.withColumn(\"is_emoji\", inputFile['total_emoji_tokens'] > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emoji Analysis\n",
    "agg_data = inputFile.groupBy(\"year\", \"month\").agg(\n",
    "                    count('user1').alias(\"total_transactions_per_month\"),\n",
    "                    sum('total_tokens').alias(\"total_tokens_per_month\"),\n",
    "                    sum('total_emoji_tokens').alias(\"total_emoji_tokens_per_month\"),\n",
    "                    (F.sum(F.col(\"if_emoji_only\").cast(\"long\")).alias(\"total_emoji_only_per_month\"))).orderBy(\"year\", \"month\")\n",
    "\n",
    "\n",
    "agg_data = agg_data.withColumn('percent_of_emoji', (F.col(\"total_emoji_tokens_per_month\") / F.col(\"total_tokens_per_month\")))           \n",
    "agg_data = agg_data.withColumn('percent_of_emoji_only', (F.col(\"total_emoji_only_per_month\") / F.col(\"total_transactions_per_month\")))                     \n",
    "agg_data = agg_data.sort('year', 'month', ascending=True)\n",
    "\n",
    "user_averageEmoji = inputFile.groupBy(\"year\", \"month\", \"user1\").agg(\n",
    "                    count('user1').alias(\"total_transactions_per_month\"),\n",
    "                    (F.sum(F.col(\"is_emoji\").cast(\"long\")).alias(\"total_emoji_transactions_per_month\")))    \n",
    "\n",
    "user_averageEmoji = user_averageEmoji.withColumn('emoji_avg',user_averageEmoji['total_emoji_transactions_per_month']/user_averageEmoji['total_transactions_per_month'])\n",
    "\n",
    "### We need this as output ###\n",
    "plot_user_emoji = user_averageEmoji.groupBy(\"year\", \"month\").agg(\n",
    "    avg(\"emoji_avg\").alias(\"avg_emoji_usage\"),\n",
    "stddev_pop(\"emoji_avg\").alias(\"sd_emoji_usage\"))\n",
    "plot_user_emoji = plot_user_emoji.sort('year', 'month', ascending=True)\n",
    "\n",
    "\n",
    "first_emoji_date = inputFile.filter(col(\"total_emoji_tokens\") > 0).groupBy(\"user1\").agg(min('datetime'))\n",
    "first_emoji_date = first_emoji_date.withColumn(\"year\", year(\"min(datetime)\"))\n",
    "first_emoji_date = first_emoji_date.withColumn(\"month\", month(\"min(datetime)\"))\n",
    "\n",
    "plot_first_emoji = first_emoji_date.groupBy(\"year\", \"month\").count().sort('year', 'month', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+--------------------+-----------------+--------------------+--------------------+------------+----+-----+--------------------+------------------------+-----------------+------------------+-------------+--------+\n",
      "|user1|user2|transaction_type|           datetime|         description|is_business|            story_id|           min_date|diff_date|customer_lifetime|  description_rm_pun|description_emoji|    description_word|         lower_words|total_tokens|year|month|     tokenized_words|tokenized_words_filtered|total_word_tokens|total_emoji_tokens|if_emoji_only|is_emoji|\n",
      "+-----+-----+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+--------------------+-----------------+--------------------+--------------------+------------+----+-----+--------------------+------------------------+-----------------+------------------+-------------+--------+\n",
      "| 2866|30588|         payment|2015-09-15 14:27:00|               Stuff|      false|55f82ab4cd03c9af2...|2015-09-15 14:27:00|        0|                0|              Stuff |                 |              Stuff |              stuff |           5|2015|    9|             [stuff]|                 [stuff]|                7|                 0|        false|   false|\n",
      "| 6620| 6507|         payment|2012-04-16 14:32:43|for taking me out...|      false|54e4165dcd03c9af2...|2012-04-16 14:32:43|        0|                0| for taking me ou...|                 | for taking me ou...| for taking me ou...|          40|2012|    4|[for, taking, me,...|    [taking, back, to...|               42|                 0|        false|   false|\n",
      "| 6620| 6606|         payment|2013-08-05 17:21:57|       Ralph LAU-ren|      false|51ffdb8f7de518fa3...|2012-04-16 14:32:43|      476|               16|      Ralph LAU ren |                 |      Ralph LAU ren |      ralph lau ren |          13|2013|    8|   [ralph, lau, ren]|       [ralph, lau, ren]|               15|                 0|        false|   false|\n",
      "|28170|27438|         payment|2012-11-03 02:35:06|           November!|      false|54e41951cd03c9af2...|2012-11-03 02:35:06|        0|                0|          November  |                 |          November  |          november  |           9|2012|   11|          [november]|              [november]|               11|                 0|        false|   false|\n",
      "|28759|65628|         payment|2013-08-11 02:55:32|               Lyft!|      false|5206fd25d56b6bac5...|2013-03-13 16:39:28|      151|                6|              Lyft  |                 |              Lyft  |              lyft  |           5|2013|    8|              [lyft]|                  [lyft]|                7|                 0|        false|   false|\n",
      "+-----+-----+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+--------------------+-----------------+--------------------+--------------------+------------+----+-----+--------------------+------------------------+-----------------+------------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input : dictionary\n",
    "emoji_dict = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/Users/yichuan/Desktop/Venmo project/data/Venmo_Emoji_Classification_Dictionary.csv\")\n",
    "word_dict = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/Users/yichuan/Desktop/Venmo project/data/Venmo_Word_Classification_Dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = word_dict.toPandas().to_dict(orient='list')\n",
    "filtered = {k: [x for x in v if x is not None] for k, v in new_dict.items()}\n",
    "new_dict.clear()\n",
    "new_dict.update(filtered)\n",
    "\n",
    "emoji_dict = emoji_dict.toPandas().to_dict(orient='list')\n",
    "filtered = {k: [x for x in v if x is not None] for k, v in emoji_dict.items()}\n",
    "emoji_dict.clear()\n",
    "emoji_dict.update(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in emoji_dict.keys():\n",
    "    new_dict[col]=new_dict[col]+ emoji_dict[col]\n",
    "    \n",
    "inputFile = inputFile.withColumn(\"Food_emoji\", classify_Food_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Event_emoji\", classify_Event_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"People_emoji\", classify_People_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Activity_emoji\", classify_Activity_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Travel_emoji\", classify_Travel_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Transportation_emoji\", classify_Transportation_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Utility_emoji\", classify_Utility_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Food_word\", classify_Food_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Event_word\", classify_Event_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"People_word\", classify_People_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Activity_word\", classify_Activity_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Travel_word\", classify_Travel_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Transportation_word\", classify_Transportation_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Utility_word\", classify_Utility_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Cash_word\", classify_Cash_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Illegal_word\", classify_Illegal_udf(\"tokenized_words_filtered\"))\n",
    "\n",
    "inputFile = inputFile.withColumn(\"Event\", F.col(\"Event_emoji\")+F.col('Event_word'))\n",
    "inputFile = inputFile.withColumn(\"Travel\", F.col(\"Travel_emoji\")+F.col('Travel_word'))\n",
    "inputFile = inputFile.withColumn(\"Food\", F.col(\"Food_emoji\")+F.col('Food_word'))\n",
    "inputFile = inputFile.withColumn(\"Activity\", F.col(\"Activity_emoji\")+F.col('Activity_word'))\n",
    "inputFile = inputFile.withColumn(\"Transportation\", F.col(\"Transportation_emoji\")+F.col('Transportation_word'))\n",
    "inputFile = inputFile.withColumn(\"People\", F.col(\"People_emoji\")+F.col('People_word'))\n",
    "inputFile = inputFile.withColumn(\"Utility\", F.col(\"Utility_emoji\")+F.col('Utility_word'))\n",
    "inputFile = inputFile.withColumn(\"Event\",  convert_dummy_udf(\"Event\"))\n",
    "inputFile = inputFile.withColumn(\"Travel\",  convert_dummy_udf(\"Travel\"))\n",
    "inputFile = inputFile.withColumn(\"Food\",  convert_dummy_udf(\"Food\"))\n",
    "inputFile = inputFile.withColumn(\"Activity\",  convert_dummy_udf(\"Activity\"))\n",
    "inputFile = inputFile.withColumn(\"Transportation\",  convert_dummy_udf(\"Transportation\"))\n",
    "inputFile = inputFile.withColumn(\"People\",  convert_dummy_udf(\"People\"))\n",
    "inputFile = inputFile.withColumn(\"Utility\",  convert_dummy_udf(\"Utility\"))\n",
    "inputFile = inputFile.withColumn(\"Total_Sum_Category_Dummies\", F.col(\"Event\")+ F.col('Travel') + F.col('Food') +F.col('Activity') + F.col('Transportation') + F.col('People') + F.col('Utility') + F.col('Illegal_word') + F.col('Cash_word'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output file: number of classified (group by month)\n",
    "\n",
    "agg_number_of_classified = inputFile.groupBy(\"year\", \"month\").agg(\n",
    "                    count('user1').alias(\"Total_Transactions_per_month\"),\n",
    "                    sum('Total_Sum_Category_Dummies').alias(\"Total_Classified_per_month\")).orderBy(\"year\", \"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------------------+--------------------------+\n",
      "|year|month|Total_Transactions_per_month|Total_Classified_per_month|\n",
      "+----+-----+----------------------------+--------------------------+\n",
      "|2011|   11|                           1|                         1|\n",
      "|2011|   12|                           4|                         2|\n",
      "|2012|    3|                           1|                         1|\n",
      "|2012|    4|                         202|                       136|\n",
      "|2012|    5|                         288|                       200|\n",
      "|2012|    6|                         295|                       220|\n",
      "|2012|    7|                         326|                       213|\n",
      "|2012|    8|                         530|                       352|\n",
      "|2012|    9|                         679|                       494|\n",
      "|2012|   10|                         829|                       548|\n",
      "+----+-----+----------------------------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_number_of_classified.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### avg across user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. for venmo time\n",
    "\n",
    "agg_categories_1 = inputFile.groupBy('user1',\"year\", \"month\").agg(\n",
    "                    count('user1').alias(\"user_total_transactions\"),\n",
    "                    sum('Total_Sum_Category_Dummies').alias(\"user_total_dummies\"),\n",
    "                    \n",
    "                    sum('Event').alias(\"Events_per_month\"),\n",
    "                    sum('Travel').alias(\"Travel_per_month\"),\n",
    "                    sum('Food').alias(\"Food_per_month\"),\n",
    "                    sum('Activity').alias(\"Activity_per_month\"),\n",
    "                    sum('Transportation').alias(\"Transportation_per_month\"),\n",
    "                    sum('People').alias(\"People_per_month\"),\n",
    "                    sum('Utility').alias(\"Utility_per_month\"),\n",
    "                    sum('Illegal_word').alias(\"Illegal_word_per_month\"),\n",
    "                    sum('Cash_word').alias(\"Cash_word_per_month\")).orderBy(\"year\", \"month\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+-----------------------+------------------+----------------+----------------+--------------+------------------+------------------------+----------------+-----------------+----------------------+-------------------+\n",
      "|user1|year|month|user_total_transactions|user_total_dummies|Events_per_month|Travel_per_month|Food_per_month|Activity_per_month|Transportation_per_month|People_per_month|Utility_per_month|Illegal_word_per_month|Cash_word_per_month|\n",
      "+-----+----+-----+-----------------------+------------------+----------------+----------------+--------------+------------------+------------------------+----------------+-----------------+----------------------+-------------------+\n",
      "|45688|2011|   11|                      1|                 1|               1|               0|             0|                 0|                       0|               0|                0|                     0|                  0|\n",
      "|73992|2011|   12|                      1|                 0|               0|               0|             0|                 0|                       0|               0|                0|                     0|                  0|\n",
      "|66737|2011|   12|                      1|                 1|               0|               0|             0|                 0|                       0|               1|                0|                     0|                  0|\n",
      "|68725|2011|   12|                      2|                 1|               0|               0|             0|                 0|                       0|               0|                1|                     0|                  0|\n",
      "|93128|2012|    3|                      1|                 1|               0|               1|             0|                 0|                       0|               0|                0|                     0|                  0|\n",
      "+-----+----+-----+-----------------------+------------------+----------------+----------------+--------------+------------------+------------------------+----------------+-----------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_categories_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of 'food'... for each user (monthly)\n",
    "agg_categories_1=agg_categories_1 \\\n",
    ".withColumn('Event',F.col('Events_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Travel',F.col('Travel_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Food',F.col('Food_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Activity',F.col('Activity_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Transportation',F.col('Transportation_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('People',F.col('People_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Utility',F.col('Utility_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Illegal',F.col('Illegal_word_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Cash',F.col('Cash_word_per_month')/F.col('user_total_dummies')) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. for customer life time\n",
    "\n",
    "agg_categories_customer_1 = inputFile.groupBy('user1',\"customer_lifetime\").agg(\n",
    "                    count('user1').alias(\"user_total_transactions\"),\n",
    "                    sum('Total_Sum_Category_Dummies').alias(\"user_total_dummies\"),\n",
    "                    \n",
    "                    sum('Event').alias(\"Events_per_month\"),\n",
    "                    sum('Travel').alias(\"Travel_per_month\"),\n",
    "                    sum('Food').alias(\"Food_per_month\"),\n",
    "                    sum('Activity').alias(\"Activity_per_month\"),\n",
    "                    sum('Transportation').alias(\"Transportation_per_month\"),\n",
    "                    sum('People').alias(\"People_per_month\"),\n",
    "                    sum('Utility').alias(\"Utility_per_month\"),\n",
    "                    sum('Illegal_word').alias(\"Illegal_word_per_month\"),\n",
    "                    sum('Cash_word').alias(\"Cash_word_per_month\")).orderBy(\"customer_lifetime\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of 'food'... for each user (monthly)\n",
    "agg_categories_customer_1 = agg_categories_customer_1 \\\n",
    ".withColumn('Event',F.col('Events_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Travel',F.col('Travel_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Food',F.col('Food_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Activity',F.col('Activity_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Transportation',F.col('Transportation_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('People',F.col('People_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Utility',F.col('Utility_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Illegal',F.col('Illegal_word_per_month')/F.col('user_total_dummies')) \\\n",
    ".withColumn('Cash',F.col('Cash_word_per_month')/F.col('user_total_dummies')) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agg_categories = agg_categories_1.groupBy(\"year\", \"month\").agg(\n",
    "                    sum('user_total_transactions').alias(\"total_transactions_per_month\"),  ## sum of 'user_total_transaction'\n",
    "                    sum('user_total_dummies').alias(\"total_dummies_per_month\"),  ## sum of 'user_total_dummies'\n",
    "                    \n",
    "                    avg('Event').alias(\"Events_per_month\"),\n",
    "                    avg('Travel').alias(\"Travel_per_month\"),\n",
    "                    avg('Food').alias(\"Food_per_month\"),\n",
    "                    avg('Activity').alias(\"Activity_per_month\"),\n",
    "                    avg('Transportation').alias(\"Transportation_per_month\"),\n",
    "                    avg('People').alias(\"People_per_month\"),\n",
    "                    avg('Utility').alias(\"Utility_per_month\"),\n",
    "                    avg('Illegal').alias(\"Illegal_word_per_month\"),\n",
    "                    avg('Cash').alias(\"Cash_word_per_month\")).orderBy(\"year\", \"month\")\n",
    "\n",
    "\n",
    "agg_categories_customer_lifetime = agg_categories_customer_1.groupBy(\"customer_lifetime\").agg(\n",
    "                    sum('user_total_transactions').alias(\"total_transactions_per_customer_stage\"),\n",
    "                    sum('user_total_dummies').alias(\"total_dummies_per_customer_stage\"),\n",
    "                    \n",
    "                    avg('Event').alias(\"Events_per_customer_stage\"),\n",
    "                    avg('Travel').alias(\"Travel_per_customer_stage\"),\n",
    "                    avg('Food').alias(\"Food_per_customer_stage\"),\n",
    "                    avg('Activity').alias(\"Activity_per_customer_stage\"),\n",
    "                    avg('Transportation').alias(\"Transportation_per_customer_stage\"),\n",
    "                    avg('People').alias(\"People_per_customer_stage\"),\n",
    "                    avg('Utility').alias(\"Utility_per_customer_stage\"),\n",
    "                    avg('Illegal').alias(\"Illegal_word_per_customer_stage\"),\n",
    "                    avg('Cash').alias(\"Cash_word_per_customer_stage\")).orderBy(\"customer_lifetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------------------+-----------------------+----------------+------------------+------------------+-------------------+------------------------+-------------------+-------------------+----------------------+-------------------+\n",
      "|year|month|total_transactions_per_month|total_dummies_per_month|Events_per_month|  Travel_per_month|    Food_per_month| Activity_per_month|Transportation_per_month|   People_per_month|  Utility_per_month|Illegal_word_per_month|Cash_word_per_month|\n",
      "+----+-----+----------------------------+-----------------------+----------------+------------------+------------------+-------------------+------------------------+-------------------+-------------------+----------------------+-------------------+\n",
      "|2011|   11|                           1|                      1|             1.0|               0.0|               0.0|                0.0|                     0.0|                0.0|                0.0|                   0.0|                0.0|\n",
      "|2011|   12|                           4|                      2|             0.0|               0.0|               0.0|                0.0|                     0.0|                0.5|                0.5|                   0.0|                0.0|\n",
      "|2012|    3|                           1|                      1|             0.0|               1.0|               0.0|                0.0|                     0.0|                0.0|                0.0|                   0.0|                0.0|\n",
      "|2012|    4|                         202|                    136|             0.0|0.0220125786163522|0.3852201257861635|0.12578616352201258|     0.08647798742138366|0.05660377358490566|0.19811320754716982|   0.09748427672955975|0.02830188679245283|\n",
      "|2012|    5|                         288|                    200|           0.025|            0.0375|           0.36875|0.18489583333333331|                0.078125|0.02708333333333333|            0.18125|   0.07864583333333333|            0.01875|\n",
      "+----+-----+----------------------------+-----------------------+----------------+------------------+------------------+-------------------+------------------------+-------------------+-------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_categories.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
