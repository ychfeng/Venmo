{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different with the code for cluster\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as func\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "import string\n",
    "from typing import Iterable\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP2\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"4G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sparknlp\n",
    "#from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def classify_Food(discription):\n",
    "    b=any([(x in new_dict['Food']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Food_udf=udf(classify_Food,IntegerType())\n",
    "\n",
    "def classify_Event(discription):\n",
    "    b=any([(x in new_dict['Event']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Event_udf=udf(classify_Event,IntegerType())\n",
    "\n",
    "def classify_People(discription):\n",
    "    b=any([(x in new_dict['People']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_People_udf=udf(classify_People,IntegerType())\n",
    "\n",
    "def classify_Activity(discription):\n",
    "    b=any([(x in new_dict['Activity']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Activity_udf=udf(classify_Activity,IntegerType())\n",
    "\n",
    "def classify_Travel(discription):\n",
    "    b=any([(x in new_dict['Travel']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Travel_udf=udf(classify_Travel,IntegerType())\n",
    "\n",
    "def classify_Transportation(discription):\n",
    "    b=any([(x in new_dict['Transportation']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Transportation_udf=udf(classify_Transportation,IntegerType())\n",
    "\n",
    "def classify_Utility(discription):\n",
    "    b=any([(x in new_dict['Utility']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Utility_udf=udf(classify_Utility,IntegerType())\n",
    "\n",
    "def classify_Cash(discription):\n",
    "    b=any([(x in new_dict['Cash']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Cash_udf=udf(classify_Cash,IntegerType())\n",
    "\n",
    "def classify_Illegal(discription):\n",
    "    b=any([(x in new_dict['Illegal/Sarcasm']) for x in set(discription)])\n",
    "    if b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "classify_Illegal_udf=udf(classify_Illegal,IntegerType())\n",
    "\n",
    "# convert to dummy variable\n",
    "def convert_dummy(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "convert_dummy_udf=udf(convert_dummy,IntegerType())\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/Users/yichuan/Desktop/Venmo project/data/venmoSample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = inputFile.na.fill('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_window = Window.partitionBy('user1')\n",
    "inputFile = inputFile.withColumn(\"min_date\", min(inputFile['datetime']).over(my_window))\n",
    "inputFile = inputFile.withColumn(\"diff_date\", datediff('datetime','min_date'))\n",
    "inputFile = inputFile.withColumn(\"customer_lifetime\", F.when(inputFile['diff_date']==0,0).otherwise(inputFile['diff_date']/30+1).cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----2. emoji explore -------------------------------------------------------------------------------\n",
    "# remove punctuation\n",
    "# remove punctuation first, so punctuation will not count as emoji\n",
    "punctuations = '~|`|\\!|@|#|$|%|^|&|\\*|\\(|\\)|-|\\+|=|_|\\{|\\}|\\[|\\]|;|:|\\?|\\.|,|<|>|/|\\'|\\\"'\n",
    "# remove punctuation\n",
    "inputFile=inputFile\\\n",
    ".withColumn('description_rm_pun',regexp_replace(col('description'),punctuations, ' '))\n",
    "# keep emoji\n",
    "inputFile=inputFile\\\n",
    ".withColumn('description_emoji',regexp_replace(col('description_rm_pun'),'[\\w\\s]', ''))\n",
    "# keep text\n",
    "inputFile=inputFile\\\n",
    ".withColumn('description_word',regexp_replace(col('description_rm_pun'),'[^\\w\\s]', ''))\n",
    "\n",
    "inputFile = inputFile.withColumn('lower_words', lower(col('description_word')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = inputFile.withColumn(\"total_tokens\", countTokens(col(\"description\")))\n",
    "inputFile = inputFile.withColumn(\"year\", year(\"datetime\"))\n",
    "inputFile = inputFile.withColumn(\"month\", month(\"datetime\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipeline2 = PretrainedPipeline('explain_document_ml', lang='en')\n",
    "inputFile = inputFile.withColumnRenamed('lower_words','text')\n",
    "\n",
    "# second step: create lemma\n",
    "inputFile = pipeline2.transform(inputFile)\n",
    "inputFile = inputFile.withColumn('lemma_result', inputFile['lemma.result'])\n",
    "\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover().setStopWords(englishStopWords).setInputCol(\"lemma_result\").setOutputCol(\"tokenized_words_filtered\")\n",
    "pipeline = Pipeline(stages=[stops])\n",
    "\n",
    "# Fit the pipeline to dataframe\n",
    "pipelineFit = pipeline.fit(inputFile)\n",
    "inputFile = pipelineFit.transform(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## count tokens in each transaction\n",
    "inputFile = inputFile.withColumn(\"total_word_tokens\", countTokens(col(\"description_word\"))) \n",
    "inputFile = inputFile.withColumn(\"total_emoji_tokens\", countTokens(col(\"description_emoji\"))) \n",
    "inputFile = inputFile.withColumn(\"if_emoji_only\", inputFile['total_emoji_tokens'] == inputFile['total_tokens'])\n",
    "inputFile = inputFile.withColumn(\"is_emoji\", inputFile['total_emoji_tokens'] > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Emoji Analysis\n",
    "agg_data = inputFile.groupBy(\"year\", \"month\").agg(\n",
    "                    count('user1').alias(\"total_transactions_per_month\"),\n",
    "                    sum('total_tokens').alias(\"total_tokens_per_month\"),\n",
    "                    sum('total_emoji_tokens').alias(\"total_emoji_tokens_per_month\"),\n",
    "                    (F.sum(F.col(\"if_emoji_only\").cast(\"long\")).alias(\"total_emoji_only_per_month\"))).orderBy(\"year\", \"month\")\n",
    "\n",
    "\n",
    "agg_data = agg_data.withColumn('percent_of_emoji', (F.col(\"total_emoji_tokens_per_month\") / F.col(\"total_tokens_per_month\")))           \n",
    "agg_data = agg_data.withColumn('percent_of_emoji_only', (F.col(\"total_emoji_only_per_month\") / F.col(\"total_transactions_per_month\")))                     \n",
    "agg_data = agg_data.sort('year', 'month', ascending=True)\n",
    "\n",
    "user_averageEmoji = inputFile.groupBy(\"year\", \"month\", \"user1\").agg(\n",
    "                    count('user1').alias(\"total_transactions_per_month\"),\n",
    "                    (F.sum(F.col(\"is_emoji\").cast(\"long\")).alias(\"total_emoji_transactions_per_month\")))    \n",
    "\n",
    "user_averageEmoji = user_averageEmoji.withColumn('emoji_avg',user_averageEmoji['total_emoji_transactions_per_month']/user_averageEmoji['total_transactions_per_month'])\n",
    "\n",
    "### We need this as output ###\n",
    "plot_user_emoji = user_averageEmoji.groupBy(\"year\", \"month\").agg(\n",
    "    avg(\"emoji_avg\").alias(\"avg_emoji_usage\"),\n",
    "stddev_pop(\"emoji_avg\").alias(\"sd_emoji_usage\"))\n",
    "plot_user_emoji = plot_user_emoji.sort('year', 'month', ascending=True)\n",
    "\n",
    "\n",
    "first_emoji_date = inputFile.filter(col(\"total_emoji_tokens\") > 0).groupBy(\"user1\").agg(min('datetime'))\n",
    "first_emoji_date = first_emoji_date.withColumn(\"year\", year(\"min(datetime)\"))\n",
    "first_emoji_date = first_emoji_date.withColumn(\"month\", month(\"min(datetime)\"))\n",
    "\n",
    "plot_first_emoji = first_emoji_date.groupBy(\"year\", \"month\").count().sort('year', 'month', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+--------------------+-----------------+--------------------+------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+-----------------+------------------+-------------+--------+\n",
      "|user1|  user2|transaction_type|           datetime|         description|is_business|            story_id|           min_date|diff_date|customer_lifetime|  description_rm_pun|description_emoji|    description_word|total_tokens|year|month|                text|            document|            sentence|               token|             checked|               lemma|                stem|                 pos|        lemma_result|tokenized_words_filtered|total_word_tokens|total_emoji_tokens|if_emoji_only|is_emoji|\n",
      "+-----+-------+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+--------------------+-----------------+--------------------+------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+-----------------+------------------+-------------+--------+\n",
      "| 2866|  30588|         payment|2015-09-15 14:27:00|               Stuff|      false|55f82ab4cd03c9af2...|2015-09-15 14:27:00|        0|                0|              Stuff |                 |              Stuff |           5|2015|    9|              stuff |[[document, 0, 6,...|[[document, 1, 5,...|[[token, 1, 5, st...|[[token, 1, 5, st...|[[token, 1, 5, st...|[[token, 1, 5, st...|[[pos, 1, 5, NN, ...|             [stuff]|                 [stuff]|                7|                 0|        false|   false|\n",
      "| 6620|   6507|         payment|2012-04-16 14:32:43|for taking me out...|      false|54e4165dcd03c9af2...|2012-04-16 14:32:43|        0|                0| for taking me ou...|                 | for taking me ou...|          40|2012|    4| for taking me ou...|[[document, 0, 41...|[[document, 1, 40...|[[token, 1, 3, fo...|[[token, 1, 3, fo...|[[token, 1, 3, fo...|[[token, 1, 3, fo...|[[pos, 1, 3, IN, ...|[for, take, i, ou...|    [take, back, toba...|               42|                 0|        false|   false|\n",
      "| 6620|   6606|         payment|2013-08-05 17:21:57|       Ralph LAU-ren|      false|51ffdb8f7de518fa3...|2012-04-16 14:32:43|      476|               16|      Ralph LAU ren |                 |      Ralph LAU ren |          13|2013|    8|      ralph lau ren |[[document, 0, 14...|[[document, 1, 13...|[[token, 1, 5, ra...|[[token, 1, 5, ra...|[[token, 1, 5, ra...|[[token, 1, 5, ra...|[[pos, 1, 5, NNS,...|  [ralphs, lau, ren]|      [ralphs, lau, ren]|               15|                 0|        false|   false|\n",
      "|28170|  27438|         payment|2012-11-03 02:35:06|           November!|      false|54e41951cd03c9af2...|2012-11-03 02:35:06|        0|                0|          November  |                 |          November  |           9|2012|   11|          november  |[[document, 0, 10...|[[document, 1, 8,...|[[token, 1, 8, no...|[[token, 1, 8, no...|[[token, 1, 8, no...|[[token, 1, 8, no...|[[pos, 1, 8, NNS,...|          [november]|              [november]|               11|                 0|        false|   false|\n",
      "|28759|  65628|         payment|2013-08-11 02:55:32|               Lyft!|      false|5206fd25d56b6bac5...|2013-03-13 16:39:28|      151|                6|              Lyft  |                 |              Lyft  |           5|2013|    8|              lyft  |[[document, 0, 6,...|[[document, 1, 4,...|[[token, 1, 4, ly...|[[token, 1, 4, le...|[[token, 1, 4, le...|[[token, 1, 4, le...|[[pos, 1, 4, VBD,...|             [leave]|                 [leave]|                7|                 0|        false|   false|\n",
      "|28759|  65628|         payment|2013-03-13 16:39:28|             Comcast|      false|5140ad1925ee44b9a...|2013-03-13 16:39:28|        0|                0|            Comcast |                 |            Comcast |           7|2013|    3|            comcast |[[document, 0, 8,...|[[document, 1, 7,...|[[token, 1, 7, co...|[[token, 1, 7, co...|[[token, 1, 7, co...|[[token, 1, 7, co...|[[pos, 1, 7, NN, ...|           [comcast]|               [comcast]|                9|                 0|        false|   false|\n",
      "|28759| 397277|         payment|2013-11-18 02:11:14|             Takeout|      false|52897742d56b6bac5...|2013-03-13 16:39:28|      250|                9|            Takeout |                 |            Takeout |           7|2013|   11|            takeout |[[document, 0, 8,...|[[document, 1, 7,...|[[token, 1, 7, ta...|[[token, 1, 7, ta...|[[token, 1, 7, ta...|[[token, 1, 7, ta...|[[pos, 1, 7, NN, ...|           [takeout]|               [takeout]|                9|                 0|        false|   false|\n",
      "|29894|  33385|          charge|2013-03-08 23:30:36|              Pizza!|      false|513a75ec25ee44b9a...|2013-03-08 23:30:36|        0|                0|             Pizza  |                 |             Pizza  |           6|2013|    3|             pizza  |[[document, 0, 7,...|[[document, 1, 5,...|[[token, 1, 5, pi...|[[token, 1, 5, pi...|[[token, 1, 5, pi...|[[token, 1, 5, pi...|[[pos, 1, 5, NN, ...|             [pizza]|                 [pizza]|                8|                 0|        false|   false|\n",
      "|33602| 152043|         payment|2014-06-21 00:42:50|             Royalty|      false|53a4d50a7d0b0354e...|2012-08-27 03:37:14|      663|               23|            Royalty |                 |            Royalty |           7|2014|    6|            royalty |[[document, 0, 8,...|[[document, 1, 7,...|[[token, 1, 7, ro...|[[token, 1, 7, ro...|[[token, 1, 7, ro...|[[token, 1, 7, ro...|[[pos, 1, 7, NN, ...|           [royalty]|               [royalty]|                9|                 0|        false|   false|\n",
      "|33602| 677386|         payment|2014-07-10 21:18:28|          Last night|      false|53bf03247d0b0354e...|2012-08-27 03:37:14|      682|               23|         Last night |                 |         Last night |          10|2014|    7|         last night |[[document, 0, 11...|[[document, 1, 10...|[[token, 1, 4, la...|[[token, 1, 4, la...|[[token, 1, 4, la...|[[token, 1, 4, la...|[[pos, 1, 4, JJ, ...|       [last, night]|           [last, night]|               12|                 0|        false|   false|\n",
      "|33602| 152043|         payment|2013-10-15 02:42:36|              Hannah|      false|525cab9cd56b6bac5...|2012-08-27 03:37:14|      414|               14|             Hannah |                 |             Hannah |           6|2013|   10|             hannah |[[document, 0, 7,...|[[document, 1, 6,...|[[token, 1, 6, ha...|[[token, 1, 6, ha...|[[token, 1, 6, ha...|[[token, 1, 6, ha...|[[pos, 1, 6, NN, ...|               [han]|                   [han]|                8|                 0|        false|   false|\n",
      "|33602| 152043|          charge|2014-06-13 20:38:09|                 Tux|      false|539b61317d0b0354e...|2012-08-27 03:37:14|      655|               22|                Tux |                 |                Tux |           3|2014|    6|                tux |[[document, 0, 4,...|[[document, 1, 3,...|[[token, 1, 3, tu...|[[token, 1, 3, tu...|[[token, 1, 3, tu...|[[token, 1, 3, tu...|[[pos, 1, 3, NN, ...|               [tux]|                   [tux]|                5|                 0|        false|   false|\n",
      "|33602| 137025|         payment|2014-02-06 00:33:51|         Red panties|      false|52f2d870c62cecac3...|2012-08-27 03:37:14|      528|               18|        Red panties |                 |        Red panties |          11|2014|    2|        red panties |[[document, 0, 12...|[[document, 1, 11...|[[token, 1, 3, re...|[[token, 1, 3, re...|[[token, 1, 3, re...|[[token, 1, 3, re...|[[pos, 1, 3, JJ, ...|        [red, panty]|            [red, panty]|               13|                 0|        false|   false|\n",
      "|33602| 137025|         payment|2014-02-01 04:02:40|             Massage|      false|52ec71e0c62cecac3...|2012-08-27 03:37:14|      523|               18|            Massage |                 |            Massage |           7|2014|    2|            massage |[[document, 0, 8,...|[[document, 1, 7,...|[[token, 1, 7, ma...|[[token, 1, 7, ma...|[[token, 1, 7, ma...|[[token, 1, 7, ma...|[[pos, 1, 7, NN, ...|           [massage]|               [massage]|                9|                 0|        false|   false|\n",
      "|33602| 137025|         payment|2015-10-05 01:09:28|                 Bro|      false|5611cdc9cd03c9af2...|2012-08-27 03:37:14|     1134|               38|                Bro |                 |                Bro |           3|2015|   10|                bro |[[document, 0, 4,...|[[document, 1, 3,...|[[token, 1, 3, br...|[[token, 1, 3, br...|[[token, 1, 3, br...|[[token, 1, 3, br...|[[pos, 1, 3, NN, ...|               [bro]|                   [bro]|                5|                 0|        false|   false|\n",
      "|33602| 137025|         payment|2012-08-27 03:37:14|Tickle fight comp...|      false|503aeb6ac5b30a098...|2012-08-27 03:37:14|        0|                0| Tickle fight com...|                 | Tickle fight com...|          24|2012|    8| tickle fight com...|[[document, 0, 25...|[[document, 1, 24...|[[token, 1, 6, ti...|[[token, 1, 6, ti...|[[token, 1, 6, ti...|[[token, 1, 6, ti...|[[pos, 1, 6, NN, ...|[tickle, fight, c...|    [tickle, fight, c...|               26|                 0|        false|   false|\n",
      "|33602|4292661|         payment|2015-04-19 01:19:20|Hollywood tours w...|      false|553302985d6cc8688...|2012-08-27 03:37:14|      965|               33| Hollywood tours ...|                 | Hollywood tours ...|          34|2015|    4| hollywood tours ...|[[document, 0, 35...|[[document, 1, 34...|[[token, 1, 9, ho...|[[token, 1, 9, ho...|[[token, 1, 9, ho...|[[token, 1, 9, ho...|[[pos, 1, 9, NN, ...|[hollywood, tour,...|    [hollywood, tour,...|               36|                 0|        false|   false|\n",
      "|33602| 137025|         payment|2013-07-26 03:40:02|                 Tip|      false|51f1ec0d7de518fa3...|2012-08-27 03:37:14|      333|               12|                Tip |                 |                Tip |           3|2013|    7|                tip |[[document, 0, 4,...|[[document, 1, 3,...|[[token, 1, 3, ti...|[[token, 1, 3, ti...|[[token, 1, 3, ti...|[[token, 1, 3, ti...|[[pos, 1, 3, NN, ...|               [tip]|                   [tip]|                5|                 0|        false|   false|\n",
      "|36525|  28192|         payment|2014-04-18 20:18:39|for Fishtown Ballers|      false|535188a0d546b8434...|2013-10-16 17:37:36|      184|                7| for Fishtown Bal...|                 | for Fishtown Bal...|          20|2014|    4| for fishtown bal...|[[document, 0, 21...|[[document, 1, 20...|[[token, 1, 3, fo...|[[token, 1, 3, fo...|[[token, 1, 3, fo...|[[token, 1, 3, fo...|[[pos, 1, 3, IN, ...|[for, fishtown, b...|     [fishtown, ballers]|               22|                 0|        false|   false|\n",
      "|36525| 192549|          charge|2013-10-16 17:37:36|      Big Booty Hoes|      false|525ecee2d56b6bac5...|2013-10-16 17:37:36|        0|                0|     Big Booty Hoes |                 |     Big Booty Hoes |          14|2013|   10|     big booty hoes |[[document, 0, 15...|[[document, 1, 14...|[[token, 1, 3, bi...|[[token, 1, 3, bi...|[[token, 1, 3, bi...|[[token, 1, 3, bi...|[[pos, 1, 3, JJ, ...|  [big, booty, home]|      [big, booty, home]|               16|                 0|        false|   false|\n",
      "+-----+-------+----------------+-------------------+--------------------+-----------+--------------------+-------------------+---------+-----------------+--------------------+-----------------+--------------------+------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+-----------------+------------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputFile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/Users/yichuan/Desktop/Venmo project/data/Venmo_Emoji_Classification_Dictionary.csv\")\n",
    "word_dict = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/Users/yichuan/Desktop/Venmo project/data/Venmo_Word_Classification_Dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = word_dict.toPandas().to_dict(orient='list')\n",
    "filtered = {k: [x for x in v if x is not None] for k, v in new_dict.items()}\n",
    "new_dict.clear()\n",
    "new_dict.update(filtered)\n",
    "\n",
    "emoji_dict = emoji_dict.toPandas().to_dict(orient='list')\n",
    "filtered = {k: [x for x in v if x is not None] for k, v in emoji_dict.items()}\n",
    "emoji_dict.clear()\n",
    "emoji_dict.update(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in emoji_dict.keys():\n",
    "    new_dict[col]=new_dict[col]+ emoji_dict[col]\n",
    "    \n",
    "inputFile = inputFile.withColumn(\"Food_emoji\", classify_Food_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Event_emoji\", classify_Event_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"People_emoji\", classify_People_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Activity_emoji\", classify_Activity_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Travel_emoji\", classify_Travel_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Transportation_emoji\", classify_Transportation_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Utility_emoji\", classify_Utility_udf(\"description_emoji\"))\n",
    "inputFile = inputFile.withColumn(\"Food_word\", classify_Food_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Event_word\", classify_Event_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"People_word\", classify_People_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Activity_word\", classify_Activity_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Travel_word\", classify_Travel_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Transportation_word\", classify_Transportation_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Utility_word\", classify_Utility_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Cash_word\", classify_Cash_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Illegal_word\", classify_Illegal_udf(\"tokenized_words_filtered\"))\n",
    "inputFile = inputFile.withColumn(\"Event\", F.col(\"Event_emoji\")+F.col('Event_word'))\n",
    "inputFile = inputFile.withColumn(\"Travel\", F.col(\"Travel_emoji\")+F.col('Travel_word'))\n",
    "inputFile = inputFile.withColumn(\"Food\", F.col(\"Food_emoji\")+F.col('Food_word'))\n",
    "inputFile = inputFile.withColumn(\"Activity\", F.col(\"Activity_emoji\")+F.col('Activity_word'))\n",
    "inputFile = inputFile.withColumn(\"Transportation\", F.col(\"Transportation_emoji\")+F.col('Transportation_word'))\n",
    "inputFile = inputFile.withColumn(\"People\", F.col(\"People_emoji\")+F.col('People_word'))\n",
    "inputFile = inputFile.withColumn(\"Utility\", F.col(\"Utility_emoji\")+F.col('Utility_word'))\n",
    "inputFile = inputFile.withColumn(\"Event\",  convert_dummy_udf(\"Event\"))\n",
    "inputFile = inputFile.withColumn(\"Travel\",  convert_dummy_udf(\"Travel\"))\n",
    "inputFile = inputFile.withColumn(\"Food\",  convert_dummy_udf(\"Food\"))\n",
    "inputFile = inputFile.withColumn(\"Activity\",  convert_dummy_udf(\"Activity\"))\n",
    "inputFile = inputFile.withColumn(\"Transportation\",  convert_dummy_udf(\"Transportation\"))\n",
    "inputFile = inputFile.withColumn(\"People\",  convert_dummy_udf(\"People\"))\n",
    "inputFile = inputFile.withColumn(\"Utility\",  convert_dummy_udf(\"Utility\"))\n",
    "inputFile = inputFile.withColumn(\"Total_Sum_Category_Dummies\", F.col(\"Event\")+ F.col('Travel') + F.col('Food') +F.col('Activity') + F.col('Transportation') + F.col('People') + F.col('Utility') + F.col('Illegal_word') + F.col('Cash_word'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_categories = inputFile.groupBy(\"year\", \"month\").agg(\n",
    "                    count('user1').alias(\"total_transactions_per_month\"),\n",
    "                    sum('Total_Sum_Category_Dummies').alias(\"total_dummies_per_month\"),\n",
    "                    sum('Event').alias(\"Events_per_month\"),\n",
    "                    sum('Travel').alias(\"Travel_per_month\"),\n",
    "                    sum('Food').alias(\"Food_per_month\"),\n",
    "                    sum('Activity').alias(\"Activity_per_month\"),\n",
    "                    sum('Transportation').alias(\"Transportation_per_month\"),\n",
    "                    sum('People').alias(\"People_per_month\"),\n",
    "                    sum('Utility').alias(\"Utility_per_month\"),\n",
    "                    sum('Illegal_word').alias(\"Illegal_word_per_month\"),\n",
    "                    sum('Cash_word').alias(\"Cash_word_per_month\")).orderBy(\"year\", \"month\")\n",
    "\n",
    "\n",
    "agg_categories_customer_lifetime = inputFile.groupBy(\"customer_lifetime\").agg(\n",
    "                    count('user1').alias(\"total_transactions_per_customer_stage\"),\n",
    "                    sum('Total_Sum_Category_Dummies').alias(\"total_dummies_per_customer_stage\"),\n",
    "                    sum('Event').alias(\"Events_per_customer_stage\"),\n",
    "                    sum('Travel').alias(\"Travel_per_customer_stage\"),\n",
    "                    sum('Food').alias(\"Food_per_customer_stage\"),\n",
    "                    sum('Activity').alias(\"Activity_per_month\"),\n",
    "                    sum('Transportation').alias(\"Transportation_per_customer_stage\"),\n",
    "                    sum('People').alias(\"People_per_customer_stage\"),\n",
    "                    sum('Utility').alias(\"Utility_per_customer_stage\"),\n",
    "                    sum('Illegal_word').alias(\"Illegal_word_per_customer_stage\"),\n",
    "                    sum('Cash_word').alias(\"Cash_word_per_customer_stage\")).orderBy(\"customer_lifetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o797.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 34.0 failed 1 times, most recent failure: Lost task 2.0 in stage 34.0 (TID 75, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 267, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 267, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6e1dd5ed0a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magg_categories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o797.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 34.0 failed 1 times, most recent failure: Lost task 2.0 in stage 34.0 (TID 75, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 267, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:136)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 267, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n"
     ]
    }
   ],
   "source": [
    "agg_categories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
