{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP2\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"4G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding model\n",
    "fcl_model = FastText.load('v_cleaned_fast_model_50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bge', 0.9215357303619385),\n",
       " ('sdge', 0.8941078186035156),\n",
       " ('xfinity', 0.8506375551223755),\n",
       " ('pgw', 0.8435612916946411),\n",
       " ('pepco', 0.8222055435180664),\n",
       " ('sewage', 0.8149155378341675),\n",
       " ('utils', 0.8075290322303772),\n",
       " ('peco', 0.804094135761261),\n",
       " ('entergy', 0.8040465116500854),\n",
       " ('verizon', 0.7912341952323914)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcl_model.wv.most_similar('pge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# python first\n",
    "# without np\n",
    "# scheme  longtype\n",
    "# track training(story id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('bge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=''>\n",
    "\n",
    "## manully code np.sum\n",
    "\n",
    "# solve lemma, do know the percentage of unclassified\n",
    "\n",
    "# create two graphs. one with unclassified, one unclassified\n",
    "\n",
    "# literature review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "\n",
    "def avg_vec(x):\n",
    "    \n",
    "    return np.sum([fcl_model.wv[word] for word in x],axis=0)[0:3]/len(x)\n",
    "\n",
    "udf_avg_vec = udf (avg_vec, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[numpy.ndarray, numpy.ndarray]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(fcl_model.wv[word]) for word in ['word','paris']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2988568 , -0.53881943, -0.34867784], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_vec(['word','xx'])  # avg_vec(['bbq', 'dranks']) works in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3]\n",
    "[3,2,3]\n",
    "\n",
    "[1+3,2+2,3+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vec_2(x):\n",
    "    to_list=np.array([fcl_model.wv[word] for word in x]).T.tolist()  # data type converted to float\n",
    "    t=[]\n",
    "    for i in range(200):\n",
    "        s=0\n",
    "        for j in range(len(x)):\n",
    "            s = s + to_list[i][j]\n",
    "        t.append(s/len(x))  # average\n",
    "    return [-0.08425377123057842, 0.04751288518309593, -0.00858938880264759]\n",
    "\n",
    "udf_avg_vec_2 = udf (avg_vec_2, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.08425377123057842,\n",
       " 0.04751288518309593,\n",
       " -0.00858938880264759,\n",
       " -0.0015542833134531975,\n",
       " 0.4552236460149288,\n",
       " 0.13901248201727867,\n",
       " 0.23095084819942713,\n",
       " 0.5302675329148769,\n",
       " -0.5856786668300629,\n",
       " 0.11387914046645164,\n",
       " -0.3505782214924693,\n",
       " 0.024346785619854927,\n",
       " 0.4365886002779007,\n",
       " 0.1106283962726593,\n",
       " 0.3992769569158554,\n",
       " -0.10566424392163754,\n",
       " 0.4047892354428768,\n",
       " -0.24622979760169983,\n",
       " -0.3883563205599785,\n",
       " -0.0013700667768716812,\n",
       " 0.1157653802074492,\n",
       " 0.11393531411886215,\n",
       " -0.05428747087717056,\n",
       " 0.32331433147192,\n",
       " -0.049651578068733215,\n",
       " 0.42375263338908553,\n",
       " -0.1480545848608017,\n",
       " 0.5127663612365723,\n",
       " -0.17447342723608017,\n",
       " 0.3025699220597744,\n",
       " -0.27124986331909895,\n",
       " 0.11526346951723099,\n",
       " 0.6149122100323439,\n",
       " -0.022356044501066208,\n",
       " -0.16442124545574188,\n",
       " 0.1266535483300686,\n",
       " 0.2057609036564827,\n",
       " -0.1638211258687079,\n",
       " 0.13178346306085587,\n",
       " -0.03680283669382334,\n",
       " -0.09881259873509407,\n",
       " 0.24205106496810913,\n",
       " 0.8382293730974197,\n",
       " 0.45627222396433353,\n",
       " -0.2077577468007803,\n",
       " -0.49030754528939724,\n",
       " -0.16797690209932625,\n",
       " 0.42193618789315224,\n",
       " -0.4228263720870018,\n",
       " 0.027646750211715698,\n",
       " 0.17725402116775513,\n",
       " -0.29172440990805626,\n",
       " 0.4210387393832207,\n",
       " -0.6421974524855614,\n",
       " 0.05872722342610359,\n",
       " 0.3812221363186836,\n",
       " 0.5846506357192993,\n",
       " -0.49406352639198303,\n",
       " -0.5298717767000198,\n",
       " -0.005664205178618431,\n",
       " 0.2923627942800522,\n",
       " -0.22537241131067276,\n",
       " 0.3804255798459053,\n",
       " 0.07917320728302002,\n",
       " 0.3367332573980093,\n",
       " 0.775227427482605,\n",
       " -0.4622429683804512,\n",
       " -0.4699002280831337,\n",
       " 0.20910311117768288,\n",
       " 0.2823124900460243,\n",
       " 0.2326318919658661,\n",
       " 0.09405631339177489,\n",
       " 0.009116753935813904,\n",
       " -0.21884604170918465,\n",
       " -0.09961788728833199,\n",
       " 0.09209320694208145,\n",
       " -0.14190150797367096,\n",
       " -0.4605948091484606,\n",
       " 0.14654690772294998,\n",
       " -0.00661642849445343,\n",
       " 0.15349330566823483,\n",
       " 0.3282425105571747,\n",
       " -0.0938183143734932,\n",
       " 0.19012228399515152,\n",
       " 0.03510409593582153,\n",
       " 0.047274982556700706,\n",
       " -0.39681071788072586,\n",
       " 0.16330360248684883,\n",
       " -0.08529222942888737,\n",
       " 0.11374897509813309,\n",
       " -0.09795800130814314,\n",
       " 0.18355295388028026,\n",
       " 0.3654918074607849,\n",
       " -0.07985184341669083,\n",
       " -0.231050755828619,\n",
       " -0.19504861906170845,\n",
       " 0.04919642023742199,\n",
       " 0.37077532708644867,\n",
       " 0.155209232121706,\n",
       " 0.26770569384098053,\n",
       " 0.04174595698714256,\n",
       " 0.23387373983860016,\n",
       " -0.1457468420267105,\n",
       " -0.028634898364543915,\n",
       " 0.08148364583030343,\n",
       " 0.08746274933218956,\n",
       " -0.0004420354962348938,\n",
       " 0.5560678765177727,\n",
       " -0.1034482903778553,\n",
       " -0.16208210214972496,\n",
       " -0.12171011744067073,\n",
       " 0.14110293239355087,\n",
       " -0.06639724969863892,\n",
       " -0.24734997004270554,\n",
       " -0.44421877403510734,\n",
       " 0.03819000814110041,\n",
       " -0.1937994807958603,\n",
       " -0.006813324987888336,\n",
       " -0.06762214004993439,\n",
       " 0.38087850622832775,\n",
       " 0.13996509462594986,\n",
       " 0.0358818955719471,\n",
       " -0.40809196792542934,\n",
       " 0.35748276859521866,\n",
       " 0.424422737210989,\n",
       " 0.015699585899710655,\n",
       " 0.14007969945669174,\n",
       " 0.01062023639678955,\n",
       " -0.3708661049604416,\n",
       " 0.14207815239205956,\n",
       " -0.08844755962491035,\n",
       " -0.004988161846995354,\n",
       " -0.04566577821969986,\n",
       " 0.6072866516187787,\n",
       " -0.09697062149643898,\n",
       " -0.33155396208167076,\n",
       " -0.2883019596338272,\n",
       " 0.04121270775794983,\n",
       " 0.2728422060608864,\n",
       " 0.07719898596405983,\n",
       " 0.15178269147872925,\n",
       " -0.47819124162197113,\n",
       " -0.07801022008061409,\n",
       " 0.4543127007782459,\n",
       " -0.08497506380081177,\n",
       " 0.12727384641766548,\n",
       " 0.4508048743009567,\n",
       " -0.010815214365720749,\n",
       " 0.17598074115812778,\n",
       " -0.17049098014831543,\n",
       " 0.1600298099219799,\n",
       " -0.036102741956710815,\n",
       " 0.15298598259687424,\n",
       " 0.5705508142709732,\n",
       " -0.4704934358596802,\n",
       " -0.0959366075694561,\n",
       " 0.37605737149715424,\n",
       " -0.2048611817881465,\n",
       " -0.16084811091423035,\n",
       " 0.05682276003062725,\n",
       " 0.16495720855891705,\n",
       " -0.05607897788286209,\n",
       " 0.09500830760225654,\n",
       " -0.7661176174879074,\n",
       " 0.00594228133559227,\n",
       " 0.17843057215213776,\n",
       " -0.10196389816701412,\n",
       " 0.2371418345719576,\n",
       " -0.17437127232551575,\n",
       " 0.11130224168300629,\n",
       " 0.032756419852375984,\n",
       " -0.1708623138256371,\n",
       " -0.8571011871099472,\n",
       " 0.09467610716819763,\n",
       " 0.0027110129594802856,\n",
       " -0.1998489387333393,\n",
       " -0.38630661368370056,\n",
       " -0.43497877940535545,\n",
       " 0.2786048408597708,\n",
       " 0.08372938307002187,\n",
       " 0.3845365047454834,\n",
       " 0.5245551653206348,\n",
       " 0.25295010954141617,\n",
       " -0.29739996418356895,\n",
       " 0.11046362295746803,\n",
       " 0.3061790665378794,\n",
       " -0.3914150670170784,\n",
       " 0.2035786509513855,\n",
       " -0.13584423810243607,\n",
       " 0.46109700947999954,\n",
       " 0.24686798453330994,\n",
       " 0.49806951731443405,\n",
       " 0.32990463078022003,\n",
       " -0.40214914083480835,\n",
       " -0.375729002058506,\n",
       " -0.005448698997497559,\n",
       " -0.5737841119989753,\n",
       " -0.04569511115550995,\n",
       " 0.21939530223608017,\n",
       " 0.796892412006855]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_vec_2(['asd','pge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(avg_vec_2(['asd','pge'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vec_3(x):\n",
    "    to_list = [fcl_model.wv[word] for word in x]  # data type converted to float\n",
    "    s = to_list[0]\n",
    "    for i in range(1,len(to_list)):\n",
    "        s = s + to_list[i]\n",
    "    return [x.item() for x in s/len(to_list)]\n",
    "\n",
    "udf_avg_vec_3 = udf (avg_vec_3, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24138659238815308,\n",
       " 0.5996950268745422,\n",
       " 0.058573853224515915,\n",
       " -0.6168175935745239,\n",
       " 0.025565002113580704,\n",
       " 0.5728688836097717,\n",
       " 0.17935851216316223,\n",
       " -0.1441439688205719,\n",
       " 0.07432284206151962,\n",
       " 0.09899347275495529,\n",
       " 0.345441609621048,\n",
       " 0.2326849102973938,\n",
       " -0.24058331549167633,\n",
       " 0.023019470274448395,\n",
       " 0.5045354962348938,\n",
       " -0.22476136684417725,\n",
       " 0.4858630299568176,\n",
       " -0.21154046058654785,\n",
       " -0.3174012005329132,\n",
       " -0.2455008327960968,\n",
       " 0.03267000615596771,\n",
       " -0.18798117339611053,\n",
       " 0.10645028948783875,\n",
       " -0.2728320360183716,\n",
       " -0.40475863218307495,\n",
       " -0.24462340772151947,\n",
       " 0.5908851623535156,\n",
       " 0.4094967246055603,\n",
       " 0.1952020525932312,\n",
       " -0.06582380831241608,\n",
       " 0.5985609292984009,\n",
       " 0.30316969752311707,\n",
       " 0.011126212775707245,\n",
       " 0.1878969371318817,\n",
       " -0.4445812702178955,\n",
       " 0.5633586645126343,\n",
       " -0.18002359569072723,\n",
       " 0.01381637156009674,\n",
       " -0.6921448707580566,\n",
       " -0.03157230466604233,\n",
       " 0.10539062321186066,\n",
       " 0.7220724821090698,\n",
       " -0.49176663160324097,\n",
       " -0.5345792174339294,\n",
       " -0.21481923758983612,\n",
       " -0.15781085193157196,\n",
       " 0.41545745730400085,\n",
       " 0.19079717993736267,\n",
       " 0.042486369609832764,\n",
       " 0.5845168828964233,\n",
       " 0.6948810815811157,\n",
       " 0.2894517481327057,\n",
       " -0.014813516288995743,\n",
       " 0.38571712374687195,\n",
       " 0.05430089682340622,\n",
       " 0.22862443327903748,\n",
       " -0.3075146973133087,\n",
       " -0.5035943388938904,\n",
       " 0.31754204630851746,\n",
       " 0.7275787591934204,\n",
       " -0.25621265172958374,\n",
       " 0.15976081788539886,\n",
       " -0.4337235987186432,\n",
       " -0.32897835969924927,\n",
       " -0.29017168283462524,\n",
       " 0.41427552700042725,\n",
       " -0.041771695017814636,\n",
       " 0.33944934606552124,\n",
       " -0.4364989399909973,\n",
       " -0.07563745230436325,\n",
       " -0.0966135710477829,\n",
       " -0.17022711038589478,\n",
       " -0.44652387499809265,\n",
       " -0.171410471200943,\n",
       " -0.05610314756631851,\n",
       " -0.11880195140838623,\n",
       " -0.16406404972076416,\n",
       " 0.26583540439605713,\n",
       " 0.5812957286834717,\n",
       " 0.031230799853801727,\n",
       " 0.710891842842102,\n",
       " 0.19123950600624084,\n",
       " 0.16145473718643188,\n",
       " -1.0937141180038452,\n",
       " -0.41619130969047546,\n",
       " -0.5496401786804199,\n",
       " -0.009547419846057892,\n",
       " 0.25164636969566345,\n",
       " -0.05885151028633118,\n",
       " -0.2835538387298584,\n",
       " 0.432407945394516,\n",
       " -0.17427833378314972,\n",
       " 0.2738644480705261,\n",
       " -0.37842056155204773,\n",
       " 0.3178461194038391,\n",
       " 0.20778590440750122,\n",
       " -0.5348951816558838,\n",
       " -0.9958575963973999,\n",
       " -0.8938151001930237,\n",
       " -0.42226800322532654,\n",
       " 0.40845203399658203,\n",
       " -0.4021947979927063,\n",
       " 0.07455049455165863,\n",
       " 0.3712356686592102,\n",
       " -0.15677237510681152,\n",
       " 0.6272784471511841,\n",
       " 0.42595502734184265,\n",
       " -0.26372453570365906,\n",
       " -0.1650366187095642,\n",
       " -0.23027178645133972,\n",
       " -0.2836090326309204,\n",
       " 0.00986585021018982,\n",
       " -0.005654916167259216,\n",
       " -0.3434098958969116,\n",
       " 0.8415781259536743,\n",
       " 0.3392402231693268,\n",
       " -0.11623731255531311,\n",
       " 0.3600509762763977,\n",
       " 0.24616596102714539,\n",
       " 0.5057518482208252,\n",
       " -0.2216307818889618,\n",
       " -0.3880231976509094,\n",
       " 0.4098224639892578,\n",
       " 0.09482824057340622,\n",
       " 0.04005357623100281,\n",
       " 0.5820469856262207,\n",
       " -0.049381136894226074,\n",
       " -0.49216294288635254,\n",
       " -0.4478539228439331,\n",
       " -0.06148771941661835,\n",
       " -0.5142543315887451,\n",
       " -0.009094499051570892,\n",
       " -0.5448404550552368,\n",
       " -0.13001501560211182,\n",
       " -0.36135756969451904,\n",
       " 0.3649803400039673,\n",
       " -0.6944676041603088,\n",
       " 0.22598668932914734,\n",
       " 0.2965676486492157,\n",
       " 0.7621053457260132,\n",
       " 0.07575683295726776,\n",
       " 0.545749843120575,\n",
       " -0.10772525519132614,\n",
       " 0.4945709705352783,\n",
       " -0.055854856967926025,\n",
       " 0.33659738302230835,\n",
       " 0.8334650993347168,\n",
       " -0.3615497946739197,\n",
       " -0.3875519633293152,\n",
       " -0.2933184802532196,\n",
       " -0.31254786252975464,\n",
       " -0.011041045188903809,\n",
       " 0.5712894201278687,\n",
       " 0.23029956221580505,\n",
       " -0.09862321615219116,\n",
       " -0.5678229331970215,\n",
       " -0.40153640508651733,\n",
       " 0.17241916060447693,\n",
       " 0.09274015575647354,\n",
       " -0.18902866542339325,\n",
       " 0.2534729540348053,\n",
       " 0.0923900231719017,\n",
       " 0.23578348755836487,\n",
       " 0.0504002571105957,\n",
       " -0.25473928451538086,\n",
       " -0.1035594791173935,\n",
       " -0.17850235104560852,\n",
       " 0.14889636635780334,\n",
       " -0.8072359561920166,\n",
       " 0.09388987720012665,\n",
       " -0.07489706575870514,\n",
       " 0.30854561924934387,\n",
       " -0.36108556389808655,\n",
       " 0.1702619343996048,\n",
       " 0.39225631952285767,\n",
       " -0.6680557131767273,\n",
       " -0.16895291209220886,\n",
       " -0.19568508863449097,\n",
       " 0.08595608919858932,\n",
       " -0.5400708913803101,\n",
       " 0.04702402651309967,\n",
       " -0.5092722773551941,\n",
       " 0.963947594165802,\n",
       " 0.275071382522583,\n",
       " -0.02964380383491516,\n",
       " -0.08473190665245056,\n",
       " -0.1429383009672165,\n",
       " -0.08523550629615784,\n",
       " 0.2519376873970032,\n",
       " -0.48726171255111694,\n",
       " -0.3179549276828766,\n",
       " -0.10180477052927017,\n",
       " 0.5091650485992432,\n",
       " 0.8891172409057617,\n",
       " -0.1370740830898285,\n",
       " -0.37794798612594604,\n",
       " 0.5101597309112549,\n",
       " 0.6206876039505005,\n",
       " -0.17140670120716095,\n",
       " -0.2075124979019165]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_vec_3(['face','book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vec_4(x):\n",
    "    word_array=np.array([fcl_model.wv[word] for word in x]).T\n",
    "    return [x.sum()/len(x) for x in word_array]\n",
    "\n",
    "udf_avg_vec_4 = udf (avg_vec_4, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.08425377309322357,\n",
       " 0.04751288518309593,\n",
       " -0.00858938880264759,\n",
       " -0.0015542833134531975,\n",
       " 0.4552236497402191,\n",
       " 0.13901248574256897,\n",
       " 0.23095084726810455,\n",
       " 0.5302675366401672,\n",
       " -0.5856786966323853,\n",
       " 0.11387914419174194,\n",
       " -0.3505782186985016,\n",
       " 0.024346785619854927,\n",
       " 0.4365885853767395,\n",
       " 0.1106283962726593,\n",
       " 0.3992769718170166,\n",
       " -0.10566424578428268,\n",
       " 0.4047892391681671,\n",
       " -0.24622979760169983,\n",
       " -0.3883563280105591,\n",
       " -0.0013700667768716812,\n",
       " 0.11576537787914276,\n",
       " 0.11393531411886215,\n",
       " -0.05428747087717056,\n",
       " 0.3233143389225006,\n",
       " -0.049651578068733215,\n",
       " 0.42375263571739197,\n",
       " -0.1480545848608017,\n",
       " 0.5127663612365723,\n",
       " -0.17447343468666077,\n",
       " 0.3025699257850647,\n",
       " -0.2712498605251312,\n",
       " 0.11526346951723099,\n",
       " 0.614912211894989,\n",
       " -0.022356044501066208,\n",
       " -0.16442124545574188,\n",
       " 0.1266535520553589,\n",
       " 0.2057608962059021,\n",
       " -0.16382113099098206,\n",
       " 0.13178345561027527,\n",
       " -0.03680283576250076,\n",
       " -0.09881259500980377,\n",
       " 0.24205106496810913,\n",
       " 0.8382293581962585,\n",
       " 0.4562722146511078,\n",
       " -0.20775774121284485,\n",
       " -0.4903075397014618,\n",
       " -0.16797690093517303,\n",
       " 0.42193618416786194,\n",
       " -0.4228263795375824,\n",
       " 0.027646750211715698,\n",
       " 0.17725402116775513,\n",
       " -0.29172441363334656,\n",
       " 0.42103874683380127,\n",
       " -0.6421974301338196,\n",
       " 0.05872722342610359,\n",
       " 0.381222128868103,\n",
       " 0.5846506357192993,\n",
       " -0.49406352639198303,\n",
       " -0.5298717617988586,\n",
       " -0.005664205178618431,\n",
       " 0.2923628091812134,\n",
       " -0.22537240386009216,\n",
       " 0.3804255723953247,\n",
       " 0.07917320728302002,\n",
       " 0.33673325181007385,\n",
       " 0.775227427482605,\n",
       " -0.4622429609298706,\n",
       " -0.4699002206325531,\n",
       " 0.20910310745239258,\n",
       " 0.2823124825954437,\n",
       " 0.2326318919658661,\n",
       " 0.09405631572008133,\n",
       " 0.009116753935813904,\n",
       " -0.21884603798389435,\n",
       " -0.09961788356304169,\n",
       " 0.09209320694208145,\n",
       " -0.14190150797367096,\n",
       " -0.4605948030948639,\n",
       " 0.14654690027236938,\n",
       " -0.00661642849445343,\n",
       " 0.15349330008029938,\n",
       " 0.3282425105571747,\n",
       " -0.0938183143734932,\n",
       " 0.19012227654457092,\n",
       " 0.03510409593582153,\n",
       " 0.047274984419345856,\n",
       " -0.39681071043014526,\n",
       " 0.16330359876155853,\n",
       " -0.08529222756624222,\n",
       " 0.11374897509813309,\n",
       " -0.09795799851417542,\n",
       " 0.18355295062065125,\n",
       " 0.3654918074607849,\n",
       " -0.07985184341669083,\n",
       " -0.2310507595539093,\n",
       " -0.19504861533641815,\n",
       " 0.04919642210006714,\n",
       " 0.37077534198760986,\n",
       " 0.1552092283964157,\n",
       " 0.26770567893981934,\n",
       " 0.04174595698714256,\n",
       " 0.23387373983860016,\n",
       " -0.1457468420267105,\n",
       " -0.028634898364543915,\n",
       " 0.08148364722728729,\n",
       " 0.08746275305747986,\n",
       " -0.0004420354962348938,\n",
       " 0.5560678839683533,\n",
       " -0.103448286652565,\n",
       " -0.16208210587501526,\n",
       " -0.12171011418104172,\n",
       " 0.14110293984413147,\n",
       " -0.06639724969863892,\n",
       " -0.24734997749328613,\n",
       " -0.44421878457069397,\n",
       " 0.03819000720977783,\n",
       " -0.1937994807958603,\n",
       " -0.006813324987888336,\n",
       " -0.06762214004993439,\n",
       " 0.3808785080909729,\n",
       " 0.13996508717536926,\n",
       " 0.0358818955719471,\n",
       " -0.4080919623374939,\n",
       " 0.35748276114463806,\n",
       " 0.4244227409362793,\n",
       " 0.015699585899710655,\n",
       " 0.14007970690727234,\n",
       " 0.01062023639678955,\n",
       " -0.3708661198616028,\n",
       " 0.14207814633846283,\n",
       " -0.08844755589962006,\n",
       " -0.004988161846995354,\n",
       " -0.04566577821969986,\n",
       " 0.6072866320610046,\n",
       " -0.09697061777114868,\n",
       " -0.33155396580696106,\n",
       " -0.288301944732666,\n",
       " 0.04121270775794983,\n",
       " 0.2728421986103058,\n",
       " 0.07719898223876953,\n",
       " 0.15178269147872925,\n",
       " -0.4781912565231323,\n",
       " -0.07801021635532379,\n",
       " 0.4543127119541168,\n",
       " -0.08497506380081177,\n",
       " 0.12727384269237518,\n",
       " 0.4508048892021179,\n",
       " -0.010815214365720749,\n",
       " 0.17598074674606323,\n",
       " -0.17049098014831543,\n",
       " 0.1600298136472702,\n",
       " -0.036102741956710815,\n",
       " 0.15298599004745483,\n",
       " 0.570550799369812,\n",
       " -0.4704934358596802,\n",
       " -0.0959366112947464,\n",
       " 0.37605738639831543,\n",
       " -0.20486117899417877,\n",
       " -0.16084811091423035,\n",
       " 0.0568227618932724,\n",
       " 0.1649572104215622,\n",
       " -0.05607897788286209,\n",
       " 0.09500830620527267,\n",
       " -0.7661176323890686,\n",
       " 0.00594228133559227,\n",
       " 0.17843057215213776,\n",
       " -0.10196390002965927,\n",
       " 0.23714183270931244,\n",
       " -0.17437127232551575,\n",
       " 0.11130224168300629,\n",
       " 0.032756417989730835,\n",
       " -0.1708623170852661,\n",
       " -0.8571012020111084,\n",
       " 0.09467610716819763,\n",
       " 0.0027110129594802856,\n",
       " -0.199848935008049,\n",
       " -0.38630661368370056,\n",
       " -0.43497878313064575,\n",
       " 0.2786048352718353,\n",
       " 0.08372938632965088,\n",
       " 0.3845365047454834,\n",
       " 0.5245551466941833,\n",
       " 0.25295010209083557,\n",
       " -0.29739996790885925,\n",
       " 0.11046361923217773,\n",
       " 0.30617907643318176,\n",
       " -0.3914150595664978,\n",
       " 0.2035786509513855,\n",
       " -0.13584423065185547,\n",
       " 0.46109700202941895,\n",
       " 0.24686798453330994,\n",
       " 0.49806952476501465,\n",
       " 0.32990461587905884,\n",
       " -0.40214914083480835,\n",
       " -0.3757289946079254,\n",
       " -0.005448698997497559,\n",
       " -0.5737841129302979,\n",
       " -0.04569511115550995,\n",
       " 0.21939530968666077,\n",
       " 0.7968924045562744]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_vec_4(['asd','pge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[numpy.float32, numpy.float32, numpy.float32]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in list(word_array[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[float, float, float]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x.item()) for x in list(word_array[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[int, int, float]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in [1,2,3.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9486510157585144, -0.4497179687023163, 0.718431830406189]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.item() for x in list(word_array[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4057883024215698"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_array[0].sum()/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>\n",
    "also, tried do nothing in function, just return a precalculated vector\n",
    "still failed.\n",
    "\n",
    "tried 50-dimentional vector\n",
    "okay, but time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_array=np.array([fcl_model.wv[word] for word in 'bge']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=[x.sum()/3 for x in word_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function worked if i explicitely write out the list: [0.4057883024215698, 0.12319501241048177, 2.2362521489461265...]\n",
    "# doesn't work if use 'sample'\n",
    "\n",
    "def avg_vec_6(x):\n",
    "    \n",
    "    return sample\n",
    "\n",
    "udf_avg_vec_6 = udf (avg_vec_6, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4057883024215698,\n",
       " 0.12319501241048177,\n",
       " 2.2362521489461265,\n",
       " -0.8568092981974283,\n",
       " 0.41192086537679035,\n",
       " 0.9721342722574869,\n",
       " 0.3621121247609456,\n",
       " 1.4297847747802734,\n",
       " -1.3532854715983074,\n",
       " -0.0746159553527832,\n",
       " -0.5155108769734701,\n",
       " 0.4771549304326375,\n",
       " -0.11149199803670247,\n",
       " 0.5983496109644572,\n",
       " 1.1879963080088298,\n",
       " -0.4804040590922038,\n",
       " 0.5738922754923502,\n",
       " -1.0371475219726562,\n",
       " 1.1692747275034587,\n",
       " 0.13129367431004843,\n",
       " 1.8482481638590496,\n",
       " -1.3392724990844727,\n",
       " 0.6459552447001139,\n",
       " 0.9448501269022623,\n",
       " -0.15130617221196493,\n",
       " -1.5248629252115886,\n",
       " 0.42719022432963055,\n",
       " 1.0200271606445312,\n",
       " 0.9996710618336996,\n",
       " -1.231173833211263,\n",
       " 0.21919731299082437,\n",
       " -1.3219615618387859,\n",
       " -0.3724816640218099,\n",
       " -0.951174259185791,\n",
       " 0.0842780073483785,\n",
       " -2.0554895401000977,\n",
       " 0.46862836678822833,\n",
       " 0.9280819098154703,\n",
       " 1.0581507682800293,\n",
       " -0.09282791614532471,\n",
       " -1.707273801167806,\n",
       " -0.6526512304941813,\n",
       " 0.11060977975527446,\n",
       " 1.5824848810831706,\n",
       " 0.5024405717849731,\n",
       " 0.24072702725728354,\n",
       " 0.3526847759882609,\n",
       " 0.318425993124644,\n",
       " 1.062428633371989,\n",
       " -0.048715045054753624,\n",
       " -1.2500244776407878,\n",
       " -0.5043360789616903,\n",
       " -0.5585001707077026,\n",
       " -3.240354855855306,\n",
       " -0.8470545609792074,\n",
       " 1.274317741394043,\n",
       " 1.6317504247029622,\n",
       " -0.5312856833140055,\n",
       " 0.0013844966888427734,\n",
       " 0.12217124303181966,\n",
       " -2.358735720316569,\n",
       " -0.6448256174723307,\n",
       " 0.6383557319641113,\n",
       " 0.1316677133242289,\n",
       " 1.0899354616800945,\n",
       " -1.2984920342763264,\n",
       " -0.030927397310733795,\n",
       " 0.2730860114097595,\n",
       " -0.24513177076975504,\n",
       " 1.6122983296712239,\n",
       " 0.4475538730621338,\n",
       " -0.5020039081573486,\n",
       " 0.9335511525472006,\n",
       " -0.3123130798339844,\n",
       " 0.7119024594624838,\n",
       " -1.0887759526570637,\n",
       " -0.8833556969960531,\n",
       " 0.7363620599110922,\n",
       " 0.5460538466771444,\n",
       " -0.18660545349121094,\n",
       " -0.1934200922648112,\n",
       " -0.6370867888132731,\n",
       " 0.14907058080037436,\n",
       " -0.31450217962265015,\n",
       " 0.7825989723205566,\n",
       " 0.858344316482544,\n",
       " 0.0496366818745931,\n",
       " 0.40000192324320477,\n",
       " 0.741720994313558,\n",
       " -0.5886622269948324,\n",
       " 0.9237200419108073,\n",
       " -1.3367811838785808,\n",
       " 1.3819947242736816,\n",
       " 0.919994036356608,\n",
       " 1.6362123489379883,\n",
       " -0.33529897530873615,\n",
       " -0.8220789432525635,\n",
       " 0.8792505264282227,\n",
       " 1.0651388963063557,\n",
       " 0.5978151162465414,\n",
       " 1.3711732228597004,\n",
       " -0.8346327940622965,\n",
       " -0.9685296217600504,\n",
       " 0.48382238547007245,\n",
       " 0.19060415029525757,\n",
       " 0.45080626010894775,\n",
       " 1.8885955810546875,\n",
       " 0.1843928893407186,\n",
       " 1.3094935417175293,\n",
       " 0.5401666959126791,\n",
       " 0.9707004229227701,\n",
       " 0.21197855472564697,\n",
       " 1.0126540660858154,\n",
       " 0.3146270116170247,\n",
       " -2.2671639124552407,\n",
       " 0.20975200335184732,\n",
       " 0.6135458946228027,\n",
       " 0.1143775184949239,\n",
       " 0.615845521291097,\n",
       " -1.5110007921854656,\n",
       " 0.19589916865030924,\n",
       " -1.1842333475748699,\n",
       " -1.8774375915527344,\n",
       " -0.2898521423339844,\n",
       " 0.5859714349110922,\n",
       " -0.9403560956319174,\n",
       " -0.6431883573532104,\n",
       " -0.09610671798388164,\n",
       " -1.998689333597819,\n",
       " -0.2308394511540731,\n",
       " -0.8044044971466064,\n",
       " -1.3691151936848958,\n",
       " 0.04167808095614115,\n",
       " 1.535936673482259,\n",
       " -0.05124363303184509,\n",
       " -0.022307040790716808,\n",
       " -0.3615423838297526,\n",
       " -0.5907551050186157,\n",
       " -0.9031224250793457,\n",
       " -2.571774164835612,\n",
       " 0.10581004619598389,\n",
       " 0.6695083777109782,\n",
       " 0.3799583911895752,\n",
       " -0.7145438194274902,\n",
       " 0.14846556385358176,\n",
       " -0.7130873998006185,\n",
       " -1.6540466944376628,\n",
       " -1.8796183268229167,\n",
       " 0.6706610520680746,\n",
       " -1.0112252235412598,\n",
       " 0.6365237236022949,\n",
       " 1.4802818298339844,\n",
       " -0.49046528339385986,\n",
       " -0.5691090027491251,\n",
       " -0.5476440985997518,\n",
       " 0.45878799756368,\n",
       " 0.4378286600112915,\n",
       " -1.9047107696533203,\n",
       " -0.4783183733622233,\n",
       " -0.3713187376658122,\n",
       " -0.7498239676157633,\n",
       " 0.20393373568852743,\n",
       " 1.4464130401611328,\n",
       " -2.25510311126709,\n",
       " 1.3805618286132812,\n",
       " -0.5003337462743124,\n",
       " -0.14499501387278238,\n",
       " 1.5713316599527996,\n",
       " -1.128039042154948,\n",
       " -1.1425711313883464,\n",
       " 0.07341678937276204,\n",
       " -0.778915802637736,\n",
       " 0.7120149930318197,\n",
       " -2.5624133745829263,\n",
       " -0.4467638333638509,\n",
       " 0.18413178126017252,\n",
       " -1.0164783795674641,\n",
       " -1.2942716280619304,\n",
       " -0.30963226159413654,\n",
       " 1.660346508026123,\n",
       " 0.049106448888778687,\n",
       " 3.1476949055989585,\n",
       " 1.1167961756388347,\n",
       " -1.0798309644063313,\n",
       " -0.008582611878712973,\n",
       " 0.560216506322225,\n",
       " -2.6788209279378257,\n",
       " 1.5278698603312175,\n",
       " 0.28807886441548664,\n",
       " 1.4539685249328613,\n",
       " 0.6848263740539551,\n",
       " -1.4388213157653809,\n",
       " -0.10179342826207478,\n",
       " 0.03494805097579956,\n",
       " 1.2646800676981609,\n",
       " 0.15542147556940714,\n",
       " -1.8180437088012695,\n",
       " 0.41296597321828205,\n",
       " -0.17255167166392008,\n",
       " 0.41491671403249103]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+\n",
      "|user1|user2|transaction_type|           datetime|description|is_business|story_id|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+\n",
      "|    1|    2|         payment|2015-10-01 02:35:00|       Uber|      false|     ddd|\n",
      "|    1|    2|         payment|2015-10-01 05:35:00|     Costco|      false|     ddd|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myMaualSchema=StructType([\n",
    "    StructField('user1',IntegerType(),True),\n",
    "    StructField('user2',IntegerType(),True),\n",
    "    StructField('transaction_type',StringType(),True),\n",
    "    StructField('datetime',TimestampType(),True),\n",
    "    StructField('description',StringType(),True),\n",
    "    StructField('is_business',StringType(),True),\n",
    "    StructField('story_id',StringType(),True),\n",
    "])\n",
    "\n",
    "person=spark.createDataFrame([\n",
    "    (1,2,'payment',datetime(2015, 10, 1,2,35),'Uber','false','ddd'),\n",
    "    (1,2,'payment',datetime(2015, 10, 1,5,35),'Costco','false','ddd'),\n",
    "],myMaualSchema).toDF('user1','user2', 'transaction_type', 'datetime', 'description', 'is_business', 'story_id')\n",
    "\n",
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = RegexTokenizer()\\\n",
    ".setInputCol(\"description\")\\\n",
    ".setOutputCol(\"tokenized_words\")\\\n",
    ".setPattern(\" \")\\\n",
    ".setToLowercase(True)\n",
    "\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    ".setStopWords(englishStopWords)\\\n",
    ".setInputCol(\"tokenized_words\")\\\n",
    ".setOutputCol(\"tokenized_words_filtered\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rt, stops])\n",
    "\n",
    "# Fit the pipeline to dataframe\n",
    "pipelineFit = pipeline.fit(person)\n",
    "person = pipelineFit.transform(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+---------------+------------------------+\n",
      "|user1|user2|transaction_type|           datetime|description|is_business|story_id|tokenized_words|tokenized_words_filtered|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+---------------+------------------------+\n",
      "|    1|    2|         payment|2015-10-01 02:35:00|       Uber|      false|     ddd|         [uber]|                  [uber]|\n",
      "|    1|    2|         payment|2015-10-01 05:35:00|     Costco|      false|     ddd|       [costco]|                [costco]|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+---------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person = person.withColumn(\"word_embedding\", udf_avg_vec(\"tokenized_words_filtered\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/broadcast.py\", line 113, in dump\n",
      "    pickle.dump(value, f, 2)\n",
      "OverflowError: cannot serialize a string larger than 4GiB\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize broadcast: OverflowError: cannot serialize a string larger than 4GiB",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: cannot serialize a string larger than 4GiB",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-4c949c21b140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word_embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf_avg_vec_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenized_words_filtered\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# ??? kernel died\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# and should have a minimal performance impact.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_judf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mjdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDataType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m     37\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2423\u001b[0;31m         \u001b[0mbroadcast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2424\u001b[0m         \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0mbroadcast_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jbroadcast\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sc, value, pickle_registry, path, sock_file)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m# no encryption, we can just write pickled data directly to the file from python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mbroadcast_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_broadcast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitTillDataReceived\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m    118\u001b[0m                   \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize broadcast: OverflowError: cannot serialize a string larger than 4GiB"
     ]
    }
   ],
   "source": [
    "person = person.withColumn(\"word_embedding\", udf_avg_vec_4(\"tokenized_words_filtered\")) \n",
    "# ??? kernel died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o147.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 5, localhost, executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4f7cd87c1b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o147.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 5, localhost, executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:90)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$1.apply(BatchEvalPythonExec.scala:89)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+\n",
      "|user1|user2|transaction_type|           datetime|description|is_business|story_id|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+\n",
      "|    1|    2|         payment|2015-10-01 02:35:00|       Uber|      false|     ddd|\n",
      "|    1|    2|         payment|2015-10-01 05:35:00|     Costco|      false|     ddd|\n",
      "|    1|    2|         payment|2015-12-01 02:35:00|       Uber|      false|     ddd|\n",
      "|    1|    2|         payment|2015-12-15 02:35:00|       Uber|      false|     ddd|\n",
      "|    1|    2|         payment|2015-12-19 02:35:00|       Uber|      false|     ddd|\n",
      "|    1|    2|         payment|2015-12-25 02:35:00|      Paris|      false|     ddd|\n",
      "|    2|    3|         payment|2015-11-01 02:35:00|       Uber|      false|     ddd|\n",
      "|    2|    1|         payment|2015-11-05 02:35:00|       Uber|      false|     ddd|\n",
      "|    2|    1|         payment|2015-12-09 02:35:00|       Uber|      false|     ddd|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myMaualSchema=StructType([\n",
    "    StructField('user1',IntegerType(),True),\n",
    "    StructField('user2',IntegerType(),True),\n",
    "    StructField('transaction_type',StringType(),True),\n",
    "    StructField('datetime',TimestampType(),True),\n",
    "    StructField('description',StringType(),True),\n",
    "    StructField('is_business',StringType(),True),\n",
    "    StructField('story_id',StringType(),True),\n",
    "])\n",
    "\n",
    "person=spark.createDataFrame([\n",
    "    (1,2,'payment',datetime(2015, 10, 1,2,35),'Uber','false','ddd'),\n",
    "    (1,2,'payment',datetime(2015, 10, 1,5,35),'Costco','false','ddd'),\n",
    "    (1,2,'payment',datetime(2015, 12, 1,2,35),'Uber','false','ddd'),\n",
    "    (1,2,'payment',datetime(2015, 12, 15,2,35),'Uber','false','ddd'),\n",
    "    (1,2,'payment',datetime(2015, 12, 19,2,35),'Uber','false','ddd'),\n",
    "    (1,2,'payment',datetime(2015, 12, 25,2,35),'Paris','false','ddd'),\n",
    "    (2,3,'payment',datetime(2015, 11, 1,2,35),'Uber','false','ddd'),\n",
    "    (2,1,'payment',datetime(2015, 11, 5,2,35),'Uber','false','ddd'),\n",
    "    (2,1,'payment',datetime(2015, 12, 9,2,35),'Uber','false','ddd'),\n",
    "],myMaualSchema).toDF('user1','user2', 'transaction_type', 'datetime', 'description', 'is_business', 'story_id')\n",
    "\n",
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.3 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "filename = 'Embed_Logistic_comb/A1.sav'\n",
    "log_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interface', 'computer']\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "print(common_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(common_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(sentences=common_texts)\n",
    "model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01146877,  0.05369632, -0.04157733, -0.0052736 ], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vec_7(x):\n",
    "    word_array=np.array([model.wv[word] for word in x]).T\n",
    "    return [x.sum()/len(x) for x in word_array]\n",
    "\n",
    "udf_avg_vec_7 = udf (avg_vec_7, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "countTokens = udf(lambda words: len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# user life time \n",
    "my_window = Window.partitionBy('user1')\n",
    "person = person.withColumn(\"min_date\", min(person['datetime']).over(my_window))\n",
    "person = person.withColumn(\"diff_date\", datediff('datetime','min_date'))\n",
    "person = person.withColumn(\"customer_lifetime\", when(person['diff_date']==0,0).otherwise(person['diff_date']/30+1).cast(IntegerType()))\n",
    "\n",
    "\n",
    "# remove punctuation\n",
    "punctuations = '~|`|\\!|@|#|$|%|^|&|\\*|\\(|\\)|-|\\+|=|_|\\{|\\}|\\[|\\]|;|:|\\?|\\.|,|<|>|/|\\'|\\\"'\n",
    "person=person\\\n",
    ".withColumn('description_rm_pun',regexp_replace(col('description'),punctuations, ' '))\n",
    "\n",
    "# keep emoji\n",
    "person=person\\\n",
    ".withColumn('description_emoji',regexp_replace(col('description_rm_pun'),'[\\w\\s]', ''))\n",
    "# keep text\n",
    "person=person\\\n",
    ".withColumn('description_word',regexp_replace(col('description_rm_pun'),'[^\\w\\s]', ''))\n",
    "\n",
    "# lower text\n",
    "person = person.withColumn('lower_words', lower(col('description_word')))\n",
    "\n",
    "person = person.withColumn(\"total_tokens\", countTokens(col(\"description\")))\n",
    "person = person.withColumn(\"year\", year(\"datetime\"))\n",
    "person = person.withColumn(\"month\", month(\"datetime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+\n",
      "|user1|user2|transaction_type|           datetime|description|is_business|story_id|           min_date|diff_date|customer_lifetime|description_rm_pun|description_emoji|description_word|lower_words|total_tokens|year|month|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+\n",
      "|    1|    2|         payment|2015-10-01 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|        0|                0|             Uber |                 |           Uber |      uber |           4|2015|   10|\n",
      "|    1|    2|         payment|2015-10-01 05:35:00|     Costco|      false|     ddd|2015-10-01 02:35:00|        0|                0|           Costco |                 |         Costco |    costco |           6|2015|   10|\n",
      "|    1|    2|         payment|2015-12-01 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       61|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|\n",
      "|    1|    2|         payment|2015-12-15 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       75|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|\n",
      "|    1|    2|         payment|2015-12-19 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       79|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|\n",
      "|    1|    2|         payment|2015-12-25 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       85|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|\n",
      "|    2|    3|         payment|2015-11-01 02:35:00|       Uber|      false|     ddd|2015-11-01 02:35:00|        0|                0|             Uber |                 |           Uber |      uber |           4|2015|   11|\n",
      "|    2|    1|         payment|2015-11-05 02:35:00|       Uber|      false|     ddd|2015-11-01 02:35:00|        4|                1|             Uber |                 |           Uber |      uber |           4|2015|   11|\n",
      "|    2|    1|         payment|2015-12-09 02:35:00|       Uber|      false|     ddd|2015-11-01 02:35:00|       38|                2|             Uber |                 |           Uber |      uber |           4|2015|   12|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+------------------------+\n",
      "|user1|user2|transaction_type|           datetime|description|is_business|story_id|           min_date|diff_date|customer_lifetime|description_rm_pun|description_emoji|description_word|lower_words|total_tokens|year|month|tokenized_words_filtered|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+------------------------+\n",
      "|    1|    2|         payment|2015-10-01 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|        0|                0|             Uber |                 |           Uber |      uber |           4|2015|   10|                  [uber]|\n",
      "|    1|    2|         payment|2015-10-01 05:35:00|     Costco|      false|     ddd|2015-10-01 02:35:00|        0|                0|           Costco |                 |         Costco |    costco |           6|2015|   10|                [costco]|\n",
      "|    1|    2|         payment|2015-12-01 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       61|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|\n",
      "|    1|    2|         payment|2015-12-15 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       75|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|\n",
      "|    1|    2|         payment|2015-12-19 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       79|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "def text_clean(x):\n",
    "    doc = nlp(x)\n",
    "    word_list = [token.lemma_ for token in doc]\n",
    "    words=[word for word in word_list if not nlp.vocab[word].is_stop and word.isalnum()]\n",
    "\n",
    "    return words\n",
    "udf_text_clean = udf(text_clean, ArrayType(StringType()))\n",
    "\n",
    "person = person.withColumn(\"tokenized_words_filtered\", udf_text_clean(\"lower_words\")) \n",
    "person.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec=Word2Vec(vectorSize=3, minCount=0,inputCol='tokenized_words_filtered',outputCol='word_vector')\n",
    "model=word2vec.fit(person)\n",
    "result=model.transform(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+------------------------+--------------------+\n",
      "|user1|user2|transaction_type|           datetime|description|is_business|story_id|           min_date|diff_date|customer_lifetime|description_rm_pun|description_emoji|description_word|lower_words|total_tokens|year|month|tokenized_words_filtered|         word_vector|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+------------------------+--------------------+\n",
      "|    1|    2|         payment|2015-10-01 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|        0|                0|             Uber |                 |           Uber |      uber |           4|2015|   10|                  [uber]|[-0.0027817289810...|\n",
      "|    1|    2|         payment|2015-10-01 05:35:00|     Costco|      false|     ddd|2015-10-01 02:35:00|        0|                0|           Costco |                 |         Costco |    costco |           6|2015|   10|                [costco]|[0.13169349730014...|\n",
      "|    1|    2|         payment|2015-12-01 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       61|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|[-0.0027817289810...|\n",
      "|    1|    2|         payment|2015-12-15 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       75|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|[-0.0027817289810...|\n",
      "|    1|    2|         payment|2015-12-19 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       79|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|[-0.0027817289810...|\n",
      "|    1|    2|         payment|2015-12-25 02:35:00|       Uber|      false|     ddd|2015-10-01 02:35:00|       85|                3|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|[-0.0027817289810...|\n",
      "|    2|    3|         payment|2015-11-01 02:35:00|       Uber|      false|     ddd|2015-11-01 02:35:00|        0|                0|             Uber |                 |           Uber |      uber |           4|2015|   11|                  [uber]|[-0.0027817289810...|\n",
      "|    2|    1|         payment|2015-11-05 02:35:00|       Uber|      false|     ddd|2015-11-01 02:35:00|        4|                1|             Uber |                 |           Uber |      uber |           4|2015|   11|                  [uber]|[-0.0027817289810...|\n",
      "|    2|    1|         payment|2015-12-09 02:35:00|       Uber|      false|     ddd|2015-11-01 02:35:00|       38|                2|             Uber |                 |           Uber |      uber |           4|2015|   12|                  [uber]|[-0.0027817289810...|\n",
      "+-----+-----+----------------+-------------------+-----------+-----------+--------+-------------------+---------+-----------------+------------------+-----------------+----------------+-----------+------------+----+-----+------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one vector for each observation\n",
    "# a vector is representing the array\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different with the code for cluster\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from datetime import datetime, date, timedelta\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import re\n",
    "\n",
    "from optparse import OptionParser\n",
    "\n",
    "import string\n",
    "from typing import Iterable\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
